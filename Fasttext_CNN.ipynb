{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asiyank/Persuasive-Detection-in-Indonesian-News/blob/CNN/Fasttext_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4pAC6rlpj8h"
      },
      "source": [
        "# Fasttext - BiLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgebE03Yq0UX"
      },
      "source": [
        "# Install Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxT9cMAnT3yS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfFvbPwXq12X",
        "outputId": "d1eaebb5-ad73-41d7-d90e-d7ee7a38f942"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\Asiyah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Sastrawi in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (1.0.1)\n",
            "Requirement already satisfied: nlp-id in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (0.1.15.0)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nlp-id) (1.2.2)\n",
            "Requirement already satisfied: nltk==3.8.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nlp-id) (3.8.1)\n",
            "Requirement already satisfied: wget==3.2 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nlp-id) (3.2)\n",
            "Requirement already satisfied: pytest==7.3.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nlp-id) (7.3.1)\n",
            "Requirement already satisfied: click in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nltk==3.8.1->nlp-id) (8.1.7)\n",
            "Requirement already satisfied: joblib in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nltk==3.8.1->nlp-id) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nltk==3.8.1->nlp-id) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nltk==3.8.1->nlp-id) (4.65.0)\n",
            "Requirement already satisfied: iniconfig in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from pytest==7.3.1->nlp-id) (2.0.0)\n",
            "Requirement already satisfied: packaging in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from pytest==7.3.1->nlp-id) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from pytest==7.3.1->nlp-id) (1.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from pytest==7.3.1->nlp-id) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from pytest==7.3.1->nlp-id) (2.0.1)\n",
            "Requirement already satisfied: colorama in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from pytest==7.3.1->nlp-id) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from scikit-learn==1.2.2->nlp-id) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from scikit-learn==1.2.2->nlp-id) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from scikit-learn==1.2.2->nlp-id) (2.2.0)\n",
            "Requirement already satisfied: transformers in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (4.32.1)\n",
            "Requirement already satisfied: filelock in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.9.0)\n",
            "Requirement already satisfied: colorama in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: torch in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from torch) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tensorflow in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (2.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (1.1.2)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (23.2)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow)\n",
            "  Downloading protobuf-3.19.6-cp38-cp38-win_amd64.whl.metadata (807 bytes)\n",
            "Requirement already satisfied: setuptools in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (68.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (1.14.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (1.48.2)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.6.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.3.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (7.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
            "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
            "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.1/26.4 MB 3.5 MB/s eta 0:00:08\n",
            "    --------------------------------------- 0.5/26.4 MB 6.4 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 1.0/26.4 MB 8.1 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 1.6/26.4 MB 9.1 MB/s eta 0:00:03\n",
            "   --- ------------------------------------ 2.1/26.4 MB 9.7 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 2.7/26.4 MB 10.0 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 3.2/26.4 MB 10.3 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 3.8/26.4 MB 10.5 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 4.4/26.4 MB 10.7 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 4.9/26.4 MB 10.8 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 5.5/26.4 MB 10.9 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 6.0/26.4 MB 11.0 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 6.6/26.4 MB 11.1 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 7.1/26.4 MB 11.1 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 7.7/26.4 MB 11.1 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 8.2/26.4 MB 11.2 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 9.0/26.4 MB 11.2 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 9.5/26.4 MB 11.3 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 10.1/26.4 MB 11.3 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 10.6/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 11.2/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 11.7/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 12.3/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 12.9/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 13.4/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 14.0/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 14.5/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 15.1/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 15.6/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 16.2/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 16.7/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 17.3/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 17.8/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 18.4/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 19.0/26.4 MB 12.1 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 19.5/26.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 20.1/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 20.8/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 21.4/26.4 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 21.9/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 22.5/26.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 23.1/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 23.6/26.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 24.2/26.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 24.8/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 25.3/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  25.9/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.4/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 26.4/26.4 MB 11.3 MB/s eta 0:00:00\n",
            "Downloading protobuf-3.19.6-cp38-cp38-win_amd64.whl (896 kB)\n",
            "   ---------------------------------------- 0.0/896.1 kB ? eta -:--:--\n",
            "   --------------------- ----------------- 501.8/896.1 kB 15.9 MB/s eta 0:00:01\n",
            "   --------------------------------------- 896.1/896.1 kB 11.4 MB/s eta 0:00:00\n",
            "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   -------------- ------------------------- 0.5/1.5 MB 17.2 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 1.1/1.5 MB 14.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 11.8 MB/s eta 0:00:00\n",
            "Installing collected packages: libclang, tensorflow-io-gcs-filesystem, protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "Successfully installed libclang-18.1.1 protobuf-3.19.6 tensorflow-io-gcs-filesystem-0.31.0\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from nltk.corpus import stopwords\n",
        "!pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "!pip install nlp-id\n",
        "!pip install transformers\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "!pip install torch\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xr7BkabObA1",
        "outputId": "dfeb285e-c696-469d-fc74-20d7f540ed4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textblob\n",
            "  Using cached textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: nltk>=3.8 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
            "Requirement already satisfied: colorama in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
            "Using cached textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
            "Installing collected packages: textblob\n",
            "Successfully installed textblob-0.18.0.post0\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxdk5_utpnx8"
      },
      "source": [
        "# Init Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EIpqk-EJlH5s",
        "outputId": "4bbd91bc-9f9d-440b-89d0-49a3c4f08c17"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id-berita</th>\n",
              "      <th>url</th>\n",
              "      <th>judul</th>\n",
              "      <th>content per paragraf</th>\n",
              "      <th>label-positif</th>\n",
              "      <th>label-persuasif</th>\n",
              "      <th>label-produk</th>\n",
              "      <th>perspektif-tunggal</th>\n",
              "      <th>label-berita</th>\n",
              "      <th>labels</th>\n",
              "      <th>content_lower</th>\n",
              "      <th>content_punct</th>\n",
              "      <th>content_lemma</th>\n",
              "      <th>content_token</th>\n",
              "      <th>content_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>229</td>\n",
              "      <td>https://tekno.sindonews.com/read/807727/776/pe...</td>\n",
              "      <td>Pengertian Absen Online dan Kelebihannya Diban...</td>\n",
              "      <td>Absen Online kini mulai merambah ke berbagai p...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>native ads</td>\n",
              "      <td>1</td>\n",
              "      <td>absen online kini mulai merambah ke berbagai p...</td>\n",
              "      <td>absen online kini mulai merambah ke berbagai p...</td>\n",
              "      <td>absen online kini mulai rambah ke bagai usaha ...</td>\n",
              "      <td>['absen', 'online', 'rambah', 'usaha', 'ganti'...</td>\n",
              "      <td>absen online rambah usaha ganti absen manual r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>229</td>\n",
              "      <td>https://tekno.sindonews.com/read/807727/776/pe...</td>\n",
              "      <td>Pengertian Absen Online dan Kelebihannya Diban...</td>\n",
              "      <td>Sebelum mengetahui manfaat absen online bagi p...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>native ads</td>\n",
              "      <td>1</td>\n",
              "      <td>sebelum mengetahui manfaat absen online bagi p...</td>\n",
              "      <td>sebelum mengetahui manfaat absen online bagi p...</td>\n",
              "      <td>belum tahu manfaat absen online bagi usaha mar...</td>\n",
              "      <td>['manfaat', 'absen', 'online', 'usaha', 'mari'...</td>\n",
              "      <td>manfaat absen online usaha mari ajar erti lapo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>229</td>\n",
              "      <td>https://tekno.sindonews.com/read/807727/776/pe...</td>\n",
              "      <td>Pengertian Absen Online dan Kelebihannya Diban...</td>\n",
              "      <td>Semua data tersebut selanjutnya akan diberikan...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>native ads</td>\n",
              "      <td>1</td>\n",
              "      <td>semua data tersebut selanjutnya akan diberikan...</td>\n",
              "      <td>semua data tersebut selanjutnya akan diberikan...</td>\n",
              "      <td>semua data sebut lanjut akan beri kepada hrd y...</td>\n",
              "      <td>['data', 'hrd', 'tanggung', 'gaji', 'karyawan'...</td>\n",
              "      <td>data hrd tanggung gaji karyawan data hrd rugi ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>229</td>\n",
              "      <td>https://tekno.sindonews.com/read/807727/776/pe...</td>\n",
              "      <td>Pengertian Absen Online dan Kelebihannya Diban...</td>\n",
              "      <td>Setiap hari karyawan melakukan absen dengan me...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>native ads</td>\n",
              "      <td>1</td>\n",
              "      <td>setiap hari karyawan melakukan absen dengan me...</td>\n",
              "      <td>setiap hari karyawan melakukan absen dengan me...</td>\n",
              "      <td>tiap hari karyawan laku absen dengan tulis jam...</td>\n",
              "      <td>['karyawan', 'laku', 'absen', 'tulis', 'jam', ...</td>\n",
              "      <td>karyawan laku absen tulis jam hadir tanda tang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>229</td>\n",
              "      <td>https://tekno.sindonews.com/read/807727/776/pe...</td>\n",
              "      <td>Pengertian Absen Online dan Kelebihannya Diban...</td>\n",
              "      <td>Sementara, Absen online merupakan sistem absen...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>native ads</td>\n",
              "      <td>1</td>\n",
              "      <td>sementara, absen online merupakan sistem absen...</td>\n",
              "      <td>sementara, absen online merupakan sistem absen...</td>\n",
              "      <td>sementara absen online rupa sistem absensi yan...</td>\n",
              "      <td>['absen', 'online', 'rupa', 'sistem', 'absensi...</td>\n",
              "      <td>absen online rupa sistem absensi manfaat jarin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>998</td>\n",
              "      <td>https://cnnindonesia.com/teknologi/20221203102...</td>\n",
              "      <td>Elon Musk Sebut Twitter Tutupi Cerita Kontrove...</td>\n",
              "      <td>Donald Trump yang menjadi lawan Biden sekaligu...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>berita murni</td>\n",
              "      <td>0</td>\n",
              "      <td>donald trump yang menjadi lawan biden sekaligu...</td>\n",
              "      <td>donald trump yang menjadi lawan biden sekalig...</td>\n",
              "      <td>donald trump yang jadi lawan biden sekaligus t...</td>\n",
              "      <td>['donald', 'trump', 'lawan', 'biden', 'tahana'...</td>\n",
              "      <td>donald trump lawan biden tahana coba serang bi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>998</td>\n",
              "      <td>https://cnnindonesia.com/teknologi/20221203102...</td>\n",
              "      <td>Elon Musk Sebut Twitter Tutupi Cerita Kontrove...</td>\n",
              "      <td>Dalam utas tersebut, Taibbi mengatakan bahwa a...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>berita murni</td>\n",
              "      <td>0</td>\n",
              "      <td>dalam utas tersebut, taibbi mengatakan bahwa a...</td>\n",
              "      <td>dalam utas tersebut, taibbi mengatakan bahwa a...</td>\n",
              "      <td>dalam utas sebut taibbi kata bahwa apa yang ak...</td>\n",
              "      <td>['utas', 'taibbi', 'baca', 'cuplik', 'buah', '...</td>\n",
              "      <td>utas taibbi baca cuplik buah serial dasar ribu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>998</td>\n",
              "      <td>https://cnnindonesia.com/teknologi/20221203102...</td>\n",
              "      <td>Elon Musk Sebut Twitter Tutupi Cerita Kontrove...</td>\n",
              "      <td>Namun menurut Taibbi, Twitter pelan-pelan mala...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>berita murni</td>\n",
              "      <td>0</td>\n",
              "      <td>namun menurut taibbi, twitter pelan-pelan mala...</td>\n",
              "      <td>namun menurut taibbi, twitter pelan-pelan mala...</td>\n",
              "      <td>namun turut taibbi twitter pelan malah tambah ...</td>\n",
              "      <td>['taibbi', 'twitter', 'pelan', 'halang', 'hala...</td>\n",
              "      <td>taibbi twitter pelan halang halang alat kontro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2500</th>\n",
              "      <td>998</td>\n",
              "      <td>https://cnnindonesia.com/teknologi/20221203102...</td>\n",
              "      <td>Elon Musk Sebut Twitter Tutupi Cerita Kontrove...</td>\n",
              "      <td>Taibbi juga menyebut sejumlah partai politik m...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>berita murni</td>\n",
              "      <td>0</td>\n",
              "      <td>taibbi juga menyebut sejumlah partai politik m...</td>\n",
              "      <td>taibbi juga menyebut sejumlah partai politik m...</td>\n",
              "      <td>taibbi juga sebut jumlah partai politik milik ...</td>\n",
              "      <td>['taibbi', 'partai', 'politik', 'milik', 'akse...</td>\n",
              "      <td>taibbi partai politik milik akses hadap alat c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2501</th>\n",
              "      <td>998</td>\n",
              "      <td>https://cnnindonesia.com/teknologi/20221203102...</td>\n",
              "      <td>Elon Musk Sebut Twitter Tutupi Cerita Kontrove...</td>\n",
              "      <td>Mereka bahkan memblok transmisi sendiri via pe...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>berita murni</td>\n",
              "      <td>0</td>\n",
              "      <td>mereka bahkan memblok transmisi sendiri via pe...</td>\n",
              "      <td>mereka bahkan memblok transmisi sendiri via pe...</td>\n",
              "      <td>mereka bahkan blok transmisi sendiri via pesan...</td>\n",
              "      <td>['blok', 'transmisi', 'via', 'pesan', 'langsun...</td>\n",
              "      <td>blok transmisi via pesan langsung dm buah alat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2502 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     id-berita                                                url  \\\n",
              "0          229  https://tekno.sindonews.com/read/807727/776/pe...   \n",
              "1          229  https://tekno.sindonews.com/read/807727/776/pe...   \n",
              "2          229  https://tekno.sindonews.com/read/807727/776/pe...   \n",
              "3          229  https://tekno.sindonews.com/read/807727/776/pe...   \n",
              "4          229  https://tekno.sindonews.com/read/807727/776/pe...   \n",
              "...        ...                                                ...   \n",
              "2497       998  https://cnnindonesia.com/teknologi/20221203102...   \n",
              "2498       998  https://cnnindonesia.com/teknologi/20221203102...   \n",
              "2499       998  https://cnnindonesia.com/teknologi/20221203102...   \n",
              "2500       998  https://cnnindonesia.com/teknologi/20221203102...   \n",
              "2501       998  https://cnnindonesia.com/teknologi/20221203102...   \n",
              "\n",
              "                                                  judul  \\\n",
              "0     Pengertian Absen Online dan Kelebihannya Diban...   \n",
              "1     Pengertian Absen Online dan Kelebihannya Diban...   \n",
              "2     Pengertian Absen Online dan Kelebihannya Diban...   \n",
              "3     Pengertian Absen Online dan Kelebihannya Diban...   \n",
              "4     Pengertian Absen Online dan Kelebihannya Diban...   \n",
              "...                                                 ...   \n",
              "2497  Elon Musk Sebut Twitter Tutupi Cerita Kontrove...   \n",
              "2498  Elon Musk Sebut Twitter Tutupi Cerita Kontrove...   \n",
              "2499  Elon Musk Sebut Twitter Tutupi Cerita Kontrove...   \n",
              "2500  Elon Musk Sebut Twitter Tutupi Cerita Kontrove...   \n",
              "2501  Elon Musk Sebut Twitter Tutupi Cerita Kontrove...   \n",
              "\n",
              "                                   content per paragraf label-positif  \\\n",
              "0     Absen Online kini mulai merambah ke berbagai p...             0   \n",
              "1     Sebelum mengetahui manfaat absen online bagi p...             1   \n",
              "2     Semua data tersebut selanjutnya akan diberikan...             1   \n",
              "3     Setiap hari karyawan melakukan absen dengan me...             1   \n",
              "4     Sementara, Absen online merupakan sistem absen...             1   \n",
              "...                                                 ...           ...   \n",
              "2497  Donald Trump yang menjadi lawan Biden sekaligu...            -1   \n",
              "2498  Dalam utas tersebut, Taibbi mengatakan bahwa a...             0   \n",
              "2499  Namun menurut Taibbi, Twitter pelan-pelan mala...            -1   \n",
              "2500  Taibbi juga menyebut sejumlah partai politik m...            -1   \n",
              "2501  Mereka bahkan memblok transmisi sendiri via pe...            -1   \n",
              "\n",
              "     label-persuasif label-produk perspektif-tunggal  label-berita labels  \\\n",
              "0                  0            1                  1    native ads      1   \n",
              "1                  1            1                  1    native ads      1   \n",
              "2                  0            1                  1    native ads      1   \n",
              "3                  1            0                  1    native ads      1   \n",
              "4                  1            1                  1    native ads      1   \n",
              "...              ...          ...                ...           ...    ...   \n",
              "2497               0            0                  1  berita murni      0   \n",
              "2498               0            0                  1  berita murni      0   \n",
              "2499               0            0                  0  berita murni      0   \n",
              "2500               0            1                  1  berita murni      0   \n",
              "2501               0            0                  1  berita murni      0   \n",
              "\n",
              "                                          content_lower  \\\n",
              "0     absen online kini mulai merambah ke berbagai p...   \n",
              "1     sebelum mengetahui manfaat absen online bagi p...   \n",
              "2     semua data tersebut selanjutnya akan diberikan...   \n",
              "3     setiap hari karyawan melakukan absen dengan me...   \n",
              "4     sementara, absen online merupakan sistem absen...   \n",
              "...                                                 ...   \n",
              "2497  donald trump yang menjadi lawan biden sekaligu...   \n",
              "2498  dalam utas tersebut, taibbi mengatakan bahwa a...   \n",
              "2499  namun menurut taibbi, twitter pelan-pelan mala...   \n",
              "2500  taibbi juga menyebut sejumlah partai politik m...   \n",
              "2501  mereka bahkan memblok transmisi sendiri via pe...   \n",
              "\n",
              "                                          content_punct  \\\n",
              "0     absen online kini mulai merambah ke berbagai p...   \n",
              "1     sebelum mengetahui manfaat absen online bagi p...   \n",
              "2     semua data tersebut selanjutnya akan diberikan...   \n",
              "3     setiap hari karyawan melakukan absen dengan me...   \n",
              "4     sementara, absen online merupakan sistem absen...   \n",
              "...                                                 ...   \n",
              "2497   donald trump yang menjadi lawan biden sekalig...   \n",
              "2498  dalam utas tersebut, taibbi mengatakan bahwa a...   \n",
              "2499  namun menurut taibbi, twitter pelan-pelan mala...   \n",
              "2500  taibbi juga menyebut sejumlah partai politik m...   \n",
              "2501  mereka bahkan memblok transmisi sendiri via pe...   \n",
              "\n",
              "                                          content_lemma  \\\n",
              "0     absen online kini mulai rambah ke bagai usaha ...   \n",
              "1     belum tahu manfaat absen online bagi usaha mar...   \n",
              "2     semua data sebut lanjut akan beri kepada hrd y...   \n",
              "3     tiap hari karyawan laku absen dengan tulis jam...   \n",
              "4     sementara absen online rupa sistem absensi yan...   \n",
              "...                                                 ...   \n",
              "2497  donald trump yang jadi lawan biden sekaligus t...   \n",
              "2498  dalam utas sebut taibbi kata bahwa apa yang ak...   \n",
              "2499  namun turut taibbi twitter pelan malah tambah ...   \n",
              "2500  taibbi juga sebut jumlah partai politik milik ...   \n",
              "2501  mereka bahkan blok transmisi sendiri via pesan...   \n",
              "\n",
              "                                          content_token  \\\n",
              "0     ['absen', 'online', 'rambah', 'usaha', 'ganti'...   \n",
              "1     ['manfaat', 'absen', 'online', 'usaha', 'mari'...   \n",
              "2     ['data', 'hrd', 'tanggung', 'gaji', 'karyawan'...   \n",
              "3     ['karyawan', 'laku', 'absen', 'tulis', 'jam', ...   \n",
              "4     ['absen', 'online', 'rupa', 'sistem', 'absensi...   \n",
              "...                                                 ...   \n",
              "2497  ['donald', 'trump', 'lawan', 'biden', 'tahana'...   \n",
              "2498  ['utas', 'taibbi', 'baca', 'cuplik', 'buah', '...   \n",
              "2499  ['taibbi', 'twitter', 'pelan', 'halang', 'hala...   \n",
              "2500  ['taibbi', 'partai', 'politik', 'milik', 'akse...   \n",
              "2501  ['blok', 'transmisi', 'via', 'pesan', 'langsun...   \n",
              "\n",
              "                                      content_stopwords  \n",
              "0     absen online rambah usaha ganti absen manual r...  \n",
              "1     manfaat absen online usaha mari ajar erti lapo...  \n",
              "2     data hrd tanggung gaji karyawan data hrd rugi ...  \n",
              "3     karyawan laku absen tulis jam hadir tanda tang...  \n",
              "4     absen online rupa sistem absensi manfaat jarin...  \n",
              "...                                                 ...  \n",
              "2497  donald trump lawan biden tahana coba serang bi...  \n",
              "2498  utas taibbi baca cuplik buah serial dasar ribu...  \n",
              "2499  taibbi twitter pelan halang halang alat kontro...  \n",
              "2500  taibbi partai politik milik akses hadap alat c...  \n",
              "2501  blok transmisi via pesan langsung dm buah alat...  \n",
              "\n",
              "[2502 rows x 15 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from io import BytesIO\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "#r = requests.get('https://docs.google.com/spreadsheet/ccc?key=1i3BBozbnx9HSTkMsfMHChvzYwG_2MiFB06U05gitD8U&output=xlsx')\n",
        "#data = r.content\n",
        "dataset = pd.read_excel('D:\\Asiyah/tesis/Thesis Persuasive Detection at Paragraph-Level/All Dataset\\Prepocessed (3).xlsx', dtype=str, index_col=None)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FElwUGx0pya5",
        "outputId": "22fa2968-6935-4c82-c1e3-30b32961a134"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2502"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6vwOZTaaYkN"
      },
      "source": [
        "# Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EvAHBHGab4K"
      },
      "outputs": [],
      "source": [
        "# Misalkan menggunakan `content_lemma` sebagai input\n",
        "texts = dataset['content_lemma'].values\n",
        "# Variabel target y adalah kombinasi dari empat label\n",
        "y = dataset['labels'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptzywVV-aez8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "#KFold(n_splits=’warn’, shuffle=False, random_state=None)\n",
        "kf = KFold(5, shuffle=True, random_state=0) # Use for KFold classification\n",
        "\n",
        "for train_index, validation_index in kf.split(texts):\n",
        "  #  print(\"TRAIN:\", texts[train_index], \"VALIDATION:\", texts[validation_index])\n",
        "   X_train, X_test = texts[train_index], texts[validation_index]\n",
        "   y_train, y_test = y[train_index], y[validation_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMTjXXl1aumz",
        "outputId": "d30b2d6c-403c-4521-c204-485477a85c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2002,) (2002,)\n",
            "(500,) (500,)\n"
          ]
        }
      ],
      "source": [
        "#Y = dataset['labels']\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIP0Z0mgqJKS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 2)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-zlbsZapcKR"
      },
      "source": [
        "# Word Embedding Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0B9f9bypeb5",
        "outputId": "83941159-6091-4f2a-c870-95e8d3c59faa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from gensim) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: fasttext in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from fasttext) (2.12.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from fasttext) (68.2.2)\n",
            "Requirement already satisfied: numpy in d:\\asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages (from fasttext) (1.24.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'gunzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Install Gensim\n",
        "!pip install --upgrade gensim\n",
        "!pip install fasttext\n",
        "# Download dan unzip dataset\n",
        "wrdvec_path = 'cc.id.300.bin.gz'\n",
        "#if not os.path.exists(wrdvec_path):\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.vec.gz\n",
        "!gunzip cc.id.300.vec.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE0ucR_Q7rkw"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjUYO4cx74I-",
        "outputId": "04115526-0dd8-413f-928b-71d90be54514"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10288"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_fatures = 100000\n",
        "#Tensorflow Tokenizer\n",
        "tokenizer = Tokenizer(num_words=max_fatures,split=' ')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size = len(tokenizer.index_word)+1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgi7DnbGbRYK"
      },
      "outputs": [],
      "source": [
        "# Convert text to sequence of integers\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EkT5-EdbV_t"
      },
      "outputs": [],
      "source": [
        "# Padding\n",
        "max_length = 257  # asumsi maksimal panjang sequence\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcWSHoYwbbcH",
        "outputId": "49aca94d-5e85-43a5-e1e0-fe8026ceac37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2002, 257)\n",
            "(500, 257)\n"
          ]
        }
      ],
      "source": [
        "print(X_train_pad.shape)\n",
        "print(X_test_pad.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmjnDUEb8R4R"
      },
      "outputs": [],
      "source": [
        "from textblob import Word\n",
        "num_tokens = len(tokenizer.index_word)+1\n",
        "\n",
        "def get_weights(embedding_vectors,embedding_dim):\n",
        "    global num_tokens,tokenizer\n",
        "\n",
        "    # assign vectors to words using the pretrained model embedding_vectors\n",
        "    embedding_weights = np.zeros((num_tokens,embedding_dim))\n",
        "\n",
        "    # count how many words are not assigned with the pretrained model.\n",
        "    # By default, vectors associated to words are zero vectors.\n",
        "    misses = 0\n",
        "\n",
        "    # the index in word_index starts with 1\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        vector = embedding_vectors.get(word)\n",
        "        # the word_index is ordered by word frequency\n",
        "        if i>=num_tokens :\n",
        "            break\n",
        "        elif vector is not None:\n",
        "            embedding_weights[i] = vector\n",
        "        else:\n",
        "            if len(word)<20:\n",
        "                word = Word(word)\n",
        "                word = word.spellcheck()[0][0]\n",
        "                vector = embedding_vectors.get(str(word))\n",
        "                if vector is not None:\n",
        "                    embedding_weights[i] = vector\n",
        "                else:\n",
        "                    misses +=1\n",
        "                    #print(word)\n",
        "            else:\n",
        "                misses +=1\n",
        "                #print(word)\n",
        "\n",
        "    print(f\"The number of missed words is {misses}\")\n",
        "\n",
        "    return embedding_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWqhn-KrObA7",
        "outputId": "55bfcba0-7c57-42cc-db2b-5911be2060ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "path = r\"D:\\Asiyah\\tesis\\Thesis Persuasive Detection at Paragraph-Level\\Pretrained embeddings\\cc.id.300-002.vec\"\n",
        "print(os.path.exists(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aTWXEp28VR5"
      },
      "outputs": [],
      "source": [
        "embedding_vectors_fasttext = {}\n",
        "with open(\"D:/Asiyah/tesis/Thesis Persuasive Detection at Paragraph-Level/Pretrained embeddings/cc.id.300-002.vec\",\"r\", encoding='utf-8') as file:\n",
        "    file.readline()\n",
        "    for line in file:\n",
        "        word , vector = line.split(maxsplit=1)\n",
        "        vector = np.fromstring(vector,\"float32\",sep=\" \")\n",
        "        embedding_vectors_fasttext[word] = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgxkluIi8mg-",
        "outputId": "aa89d399-1bc3-4e4d-c84c-33a12165d71d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of missed words is 695\n"
          ]
        }
      ],
      "source": [
        "# assign vectors to words using the pretrained model fasttext\n",
        "embedding_matrix = get_weights(embedding_vectors_fasttext, embedding_dim=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AZ2IvuBO14S"
      },
      "source": [
        "## Saved Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOqTbgijOuwW",
        "outputId": "deec3db9-628f-4d9a-d60b-56fcf113f49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding matrix saved to D:/Asiyah/tesis/Thesis Persuasive Detection at Paragraph-Level/Pretrained embeddings/FS_embedding.npy\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Menyimpan embedding matrix\n",
        "filename_npy = 'D:/Asiyah/tesis/Thesis Persuasive Detection at Paragraph-Level/Pretrained embeddings/FS_embedding.npy'\n",
        "# Simpan embedding matrix ke dalam file .npy\n",
        "np.save(filename_npy, embedding_matrix)\n",
        "\n",
        "print(f\"Embedding matrix saved to {filename_npy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5VJljDxxLNV"
      },
      "source": [
        "# BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulCMuEgXezSD"
      },
      "outputs": [],
      "source": [
        "from keras import Model\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import Input, Embedding, Conv1D, Bidirectional, LSTM, Dense, Dropout, BatchNormalization, GlobalMaxPooling1D,concatenate,ConvLSTM1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional\n",
        "from keras.callbacks import EarlyStopping, TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_SJ8YOKZKZF",
        "outputId": "50fe6cce-d18e-49a7-f557-aa38c5832e39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 257, 300)          3086400   \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 257, 300)         1200      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 257, 300)          0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 257, 512)         1140736   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 512)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,229,362\n",
            "Trainable params: 1,142,362\n",
            "Non-trainable params: 3,087,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "embed_dim = 300\n",
        "vocab_size = len(tokenizer.index_word)+1\n",
        "model = Sequential()\n",
        "## embedding layer\n",
        "\n",
        "model = Sequential([\n",
        "    # Lapisan embedding yang mengonversi input integer menjadi vektor dense\n",
        "    Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=257, weights=[embedding_matrix], trainable=False),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),  # Tambahkan Dropout\n",
        "    # Lapisan BiLSTM\n",
        "    Bidirectional(LSTM(256, return_sequences=True)),\n",
        "    GlobalMaxPooling1D(),  # Tambahkan GlobalMaxPooling1D\n",
        "    # Opsional: Tambahkan dropout untuk regularisasi\n",
        "    Dropout(0.2),\n",
        "    # Lapisan Dense untuk klasifikasi\n",
        "    Dense(2, activation='sigmoid')  # '4' sesuai dengan jumlah label target Anda, sesuaikan jika berbeda\n",
        "])\n",
        "\n",
        "# Ringkasan model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX4F5MUa0TXD",
        "outputId": "91023ea5-67a6-454b-9c28-a97ecd1be73b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "16/16 [==============================] - 22s 1s/step - loss: 0.4175 - accuracy: 0.8162 - val_loss: 1.0389 - val_accuracy: 0.5440\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.2465 - accuracy: 0.9006 - val_loss: 0.5160 - val_accuracy: 0.8100\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.1997 - accuracy: 0.9241 - val_loss: 0.3574 - val_accuracy: 0.8600\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.1557 - accuracy: 0.9436 - val_loss: 0.4219 - val_accuracy: 0.8420\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.1330 - accuracy: 0.9500 - val_loss: 0.6239 - val_accuracy: 0.7040\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0920 - accuracy: 0.9690 - val_loss: 0.4700 - val_accuracy: 0.8200\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0852 - accuracy: 0.9705 - val_loss: 0.4291 - val_accuracy: 0.8280\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0608 - accuracy: 0.9825 - val_loss: 0.4068 - val_accuracy: 0.8280\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0439 - accuracy: 0.9895 - val_loss: 0.4727 - val_accuracy: 0.7920\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0279 - accuracy: 0.9930 - val_loss: 1.0325 - val_accuracy: 0.6320\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0277 - accuracy: 0.9945 - val_loss: 1.1231 - val_accuracy: 0.6600\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0217 - accuracy: 0.9950 - val_loss: 1.7350 - val_accuracy: 0.5300\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0212 - accuracy: 0.9950 - val_loss: 1.0875 - val_accuracy: 0.6560\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0143 - accuracy: 0.9980 - val_loss: 1.2360 - val_accuracy: 0.5960\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.6466 - val_accuracy: 0.7740\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.6754 - val_accuracy: 0.8120\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.0671 - val_accuracy: 0.6780\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.8562 - val_accuracy: 0.7900\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.8181 - val_accuracy: 0.7980\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.9939 - val_accuracy: 0.6920\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0105 - accuracy: 0.9990 - val_loss: 0.4653 - val_accuracy: 0.8480\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0082 - accuracy: 0.9980 - val_loss: 0.7302 - val_accuracy: 0.8020\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.7175 - val_accuracy: 0.8060\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0090 - accuracy: 0.9965 - val_loss: 0.4708 - val_accuracy: 0.8920\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0107 - accuracy: 0.9980 - val_loss: 0.4013 - val_accuracy: 0.8860\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0064 - accuracy: 0.9995 - val_loss: 0.5305 - val_accuracy: 0.8880\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.5719 - val_accuracy: 0.8880\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5220 - val_accuracy: 0.8740\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5816 - val_accuracy: 0.8820\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 8.5977e-04 - accuracy: 1.0000 - val_loss: 0.5293 - val_accuracy: 0.9040\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.4778 - val_accuracy: 0.8980\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4493 - val_accuracy: 0.9000\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4945 - val_accuracy: 0.9060\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 8.9606e-04 - accuracy: 1.0000 - val_loss: 0.4806 - val_accuracy: 0.9080\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 5.6961e-04 - accuracy: 1.0000 - val_loss: 0.4593 - val_accuracy: 0.9080\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 6.1581e-04 - accuracy: 1.0000 - val_loss: 0.4900 - val_accuracy: 0.9120\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.4489 - val_accuracy: 0.9180\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.6598 - val_accuracy: 0.8920\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0052 - accuracy: 0.9980 - val_loss: 0.3896 - val_accuracy: 0.9240\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0060 - accuracy: 0.9975 - val_loss: 0.4307 - val_accuracy: 0.9180\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0072 - accuracy: 0.9975 - val_loss: 0.3371 - val_accuracy: 0.9280\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0064 - accuracy: 0.9975 - val_loss: 0.3638 - val_accuracy: 0.9320\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0093 - accuracy: 0.9975 - val_loss: 0.3847 - val_accuracy: 0.9200\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.3929 - val_accuracy: 0.9240\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3486 - val_accuracy: 0.9120\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.3306 - val_accuracy: 0.9220\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0051 - accuracy: 0.9990 - val_loss: 0.2827 - val_accuracy: 0.9240\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3206 - val_accuracy: 0.9240\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.3576 - val_accuracy: 0.9300\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.3906 - val_accuracy: 0.9240\n",
            "Accuracy: 92.40%\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # Gunakan 'sparse_categorical_crossentropy' jika label adalah integer\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_pad, y_train, epochs=50, batch_size=128, validation_data=(X_test_pad, y_test))\n",
        "score = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(\"Accuracy: {:.2f}%\".format(score[1] * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tu8E2CT9Ni0"
      },
      "source": [
        "## Get Evaluate Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCynlzBh5na5"
      },
      "outputs": [],
      "source": [
        "yhat = model.predict(X_test_pad, verbose=0)\n",
        "classes_x=np.rint(yhat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CYUNjNf5qcm"
      },
      "outputs": [],
      "source": [
        "npa = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo2wc8ia5ueu",
        "outputId": "9f801d4a-aabd-4dd3-9c4d-b938e99db62f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision :  0.9230644900598028\n",
            "Recall    :  0.9219233746130031\n",
            "F-score   :  0.922470921550743\n",
            "Accuracy :  0.922\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "precision,recall,fscore,support=score(npa,classes_x,average='macro')\n",
        "print('Precision : ',format(precision))\n",
        "print('Recall    : ',format(recall))\n",
        "print('F-score   : ',format(fscore))\n",
        "print('Accuracy : ',accuracy_score(npa, classes_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K47p9eu0mpF",
        "outputId": "535d8148-f082-4dd2-d019-8ea92fb74195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.92      0.92       228\n",
            "           1       0.93      0.92      0.93       272\n",
            "\n",
            "   micro avg       0.92      0.92      0.92       500\n",
            "   macro avg       0.92      0.92      0.92       500\n",
            "weighted avg       0.92      0.92      0.92       500\n",
            " samples avg       0.92      0.92      0.92       500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(npa, classes_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuImxomRKNgn"
      },
      "outputs": [],
      "source": [
        "# npa = np.argmax(npa, axis=-1)\n",
        "# classes_x = np.argmax(classes_x, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQfTiETl44EL",
        "outputId": "b20c6f10-d18d-4bd9-ddbc-951d201091a5"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "multilabel-indicator is not supported",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(npa)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(classes_x)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cm)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mD:\\Asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:319\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    317\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m     labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
            "\u001b[1;31mValueError\u001b[0m: multilabel-indicator is not supported"
          ]
        }
      ],
      "source": [
        "if len(np.unique(npa)) > 1 and len(np.unique(classes_x)) > 1:\n",
        "    cm = confusion_matrix(npa, classes_x)\n",
        "    print(cm)\n",
        "else:\n",
        "    print(\"Error: Data tidak mengandung cukup kelas untuk menghitung confusion matrix.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9uBOKYPVA_4",
        "outputId": "3ebb4227-325d-423f-dd3c-fea2e0b4c014"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "multilabel-indicator is not supported",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[49], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNews\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNative Ads\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m disp \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mcm, display_labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     11\u001b[0m disp\u001b[38;5;241m.\u001b[39mplot(cmap\u001b[38;5;241m=\u001b[39mplt\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mBlues)\n",
            "File \u001b[1;32mD:\\Asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:319\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    317\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m     labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
            "\u001b[1;31mValueError\u001b[0m: multilabel-indicator is not supported"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = [\"News\", \"Native Ads\"]\n",
        "\n",
        "cm = confusion_matrix(npa, classes_x)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mXepIXRerf7"
      },
      "source": [
        "## roc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmvAytw6S-sc",
        "outputId": "420aadc6-e19f-4a07-8e32-cef8c6fd8550"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "multilabel-indicator format is not supported",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[50], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(npa)\n\u001b[0;32m      6\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(classes_x)\n\u001b[1;32m----> 7\u001b[0m fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m roc_auc \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mauc(fpr, tpr)\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
            "File \u001b[1;32mD:\\Asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:992\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroc_curve\u001b[39m(\n\u001b[0;32m    905\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    906\u001b[0m ):\n\u001b[0;32m    907\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;124;03m    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 992\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
            "File \u001b[1;32mD:\\Asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:749\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    747\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m--> 749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m    751\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m    752\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
            "\u001b[1;31mValueError\u001b[0m: multilabel-indicator format is not supported"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "y = np.array(npa)\n",
        "\n",
        "scores = np.array(classes_x)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(npa, classes_x)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        " lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC1SjAu9TgMm",
        "outputId": "526e93b4-3cd1-4448-ef93-dad2c3263c0f"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "multilabel-indicator format is not supported",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_curve, auc\n\u001b[1;32m----> 3\u001b[0m fpr_rf, tpr_rf, thresholds_rf \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m auc_rf \u001b[38;5;241m=\u001b[39m auc(fpr_rf, tpr_rf)\n",
            "File \u001b[1;32mD:\\Asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:992\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroc_curve\u001b[39m(\n\u001b[0;32m    905\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    906\u001b[0m ):\n\u001b[0;32m    907\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;124;03m    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 992\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
            "File \u001b[1;32mD:\\Asiyah\\anaconda\\envs\\asiyah_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:749\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    747\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m--> 749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m    751\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m    752\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
            "\u001b[1;31mValueError\u001b[0m: multilabel-indicator format is not supported"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(npa, classes_x)\n",
        "auc_rf = auc(fpr_rf, tpr_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXYgRXq0TvXx"
      },
      "outputs": [],
      "source": [
        "auc_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ8MxCcJTdLj"
      },
      "outputs": [],
      "source": [
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "# Zoom in view of the upper left corner.\n",
        "plt.figure(2)\n",
        "plt.xlim(0, 0.2)\n",
        "plt.ylim(0.8, 1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve (zoomed in at top left)')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BkQtuMJkYfH"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curve(fpr,tpr):\n",
        "  plt.plot(fpr,tpr)\n",
        "  plt.axis([0,1,0,1])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkkP0epDkZUJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "\n",
        "fpr , tpr , thresholds = roc_curve ( npa , classes_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GglChJpdkdTz"
      },
      "outputs": [],
      "source": [
        "plot_roc_curve (fpr,tpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkSh8Qg1kiGr"
      },
      "outputs": [],
      "source": [
        "roc_auc_score(npa,classes_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEyAP6xUk_L6"
      },
      "outputs": [],
      "source": [
        "nn_fpr_keras, nn_tpr_keras, nn_thresholds_keras = roc_curve(npa, classes_x)\n",
        "auc_keras = auc(nn_fpr_keras, nn_tpr_keras)\n",
        "print(auc_keras)\n",
        "plt.plot(nn_fpr_keras, nn_tpr_keras, marker='.', label='Neural Network (auc = %0.3f)' % auc_keras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdsXWMEPptkz"
      },
      "outputs": [],
      "source": [
        "classes_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoYHcueMpxW1"
      },
      "outputs": [],
      "source": [
        "npa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFnVYJHvqTMd"
      },
      "outputs": [],
      "source": [
        "def perf_measure(y_actual, y_hat):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "\n",
        "    for i in range(len(y_hat)):\n",
        "        if y_actual[i]==y_hat[i]==1:\n",
        "           TP += 1\n",
        "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
        "           FP += 1\n",
        "        if y_actual[i]==y_hat[i]==0:\n",
        "           TN += 1\n",
        "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "           FN += 1\n",
        "\n",
        "    return(TP, FP, TN, FN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIZaT7cUqeLg"
      },
      "outputs": [],
      "source": [
        "perf_measure(npa,classes_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Dz46n58qjMl"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9cotOpCObBC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}